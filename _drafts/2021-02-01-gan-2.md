---
layout: post
title:  "Gentle Intro to the Generative Adversarial Networks (GANs)"
---

## What can a GAN do?

GAN is about generating data from scratch, like an artist. The modern GAN usage would involve generating data, like composing a symphony, or drawing a landscape. Thousands of GAN research papers were published in recent years, with broad areas ranging from game development, medical imaging, text-to-image translation, etc.

## Some Preliminaries

**Def. Probabilistic Generative Model**
In short, a generative model is a statistical model of the joint probability $$P(X,Y)$$ on given observed variable $$X$$ and target variable $$Y$$.

Consider this generative approach for mdeling the class-conditional densities $$p(\mathbf{x}\>|\>\mathcal{C}_k)$$, as well as the class priors $$p(\mathcal{C}_k)$$, then we use them to compute posterior probabilities $$p(\mathcal{C}_k>|\>\mathbf{x})$$ using Baye's Theorem.

Consider first of all the case of two classes. Then the posterior probability for class $$\mathcal{C}_1$$ can be written as

$$
\begin{align}
  p(\mathcal{C}_1\>|\>\mathbf{x})&=\frac{p(\mathbf{x}\>|\>\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x}\>|\>\mathcal{C}_1)p(\mathcal{C}_1)p(\mathbf{x}\>|\>\mathcal{C}_2)p(\mathcal{C}_2)}\\
  &=\frac{1}{1+\exp(-a)}\\
  &=\sigma(a)
\end{align}
$$

where

* $$a$$ is defined as $$\log(p(\mathbf{x}\>|\>\mathcal{C}_1)p(\mathcal{C}_1))/p(\mathbf{x}\>|\>\mathcal{C}_2)p(\mathcal{C}_2)$$.

* $$\sigma$$ is the *logistic sigmoid function*.

**Def. Discriminative Model**
A discriminative model is a

## How do the Adversarial Nets work?

The GAN provides a framework for **estimating generative models** through an **adversarial** process. In this framework we train the following two models simultaneously:

* $$G$$ - Generative model that captures the data distribution.
* $$D$$ - Discriminative model that estimates the probability that a sample is from the training data, rather than $$G$$.

To learn $$G$$'s generated distrbution $$p_g$$ from data input $$\mathbf{x}$$, we define a prior on the input noise variables $$p_{\mathbf{z}}(\mathbf{z})$$, then we use a differentiable function $$G$$ to map $$\mathbf{z}$$ to the data space as $$G(\mathbf{z};\theta_g)$$.

* Here $$G$$ is being represented by a multilayer perceptron with parameters $$\theta_g$$.

Additionally, we define another multilayer perceptron $$D(\mathbf{x};\theta_d)$$ that outputs a scalar.

* Here $$D(\mathbf{x})$$ represents the probability that $$\mathbf{x}$$ coming from the data, rather than the generated $$p_g$$.

Finally, our goal is to train $$D$$ to maximize the probability of assigning the correct label to **both** samples from $$G$$ and training examples. Therefore, we will train $$G$$ to **minimize** $$\log(1-D(G(\mathbf{z})))$$. This yields a *two-player minimax game* with value function $$V(G,D)$$:

$$
\min_G\max_D V(D,G)=\mathbb{E}_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})]+\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}(\mathbf{z})}[\log(1-D(G(\mathbf{z})))].
$$

Note that in the function space of arbitrary $$G$$ and $$D$$, there exists a **unique** solution, in which $$G$$ recovers the training data distribution, and $$D$$ will be constantly $$1/2$$.

If $$G$$ and $$D$$ are defined as *multilayer perceptrons*, we then are able to train the system using *backpropagation*.

## An Analogy...

To view this in a analogous way, try to think in the following way:

* Consider the generative model $$G$$ as a group of counterfeiters trying to produce fake paintings without being detected.

{:refdef: style="text-align: center;"}
![Thief](https://www.drawinghowtodraw.com/stepbystepdrawinglessons/wp-content/uploads/2011/02/06-thief-color.png){:width="250px"}
{:refdef}

* Consider the discriminative model $$D$$ as a group of police trying to detect the fake paintings.

{:refdef: style="text-align: center;"}
![Mona Lisa](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg/1920px-Mona_Lisa%2C_by_Leonardo_da_Vinci%2C_from_C2RMF_retouched.jpg){:width="250px" centered}
{:refdef}

Then the competition will drive both groups to improve their methods until the counterfeits draw paintings that are not disguinshable from the actual paintings anymore.

## Example Showcase

![Putin]({{ site.base_url }}/data/gan-example-1.gif)

## Related work

[Check out Ian Goodfellow's tutorial workshop.](https://youtu.be/HGYYEUSm-0Q)

The following image illustrates the DCGAN, one of the most popular generator network design, which performes multiple transposed convolutions to upsample $$\mathbf{z}$$ to generate the data $$\mathbf{x}$$ (here, an image).

![DCGAN](https://miro.medium.com/max/1400/1*ULAGAYoGGr5eEB2377ArYA.png)

## References

[1] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks. ArXiv:1406.2661 [Cs, Stat]. http://arxiv.org/abs/1406.2661

[2] Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.
