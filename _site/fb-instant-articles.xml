<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      Ziyue Yang's Personal Website
    </description>
    
        
            <item>
                <title>The Most Interesting Books I Read in 2019</title>
                <link>http://localhost:4000/2019/12/31/books/</link>
                <content:encoded>
                    <![CDATA[
                    <p>I used to read a lot in high school, but between studying, joining clubs, and meeting
new people, I completely lost the habit in my first two years of university.
This summer I had an hour long commute each way on the subway, giving me the perfect
opportunity to start reading again. Here are the books I liked reading the most this year, in
the opposite order I finished reading them.</p>

<hr />

<h2 id="1-finding-meaning-in-an-imperfect-world-iddo-landau">1. <em>Finding Meaning in an Imperfect World</em>, Iddo Landau</h2>

<p>One of the most important books to me is <em>Man’s Search for Meaning</em> by Viktor Frankl.
In it, he talks about finding meaning through alleviating suffering in others.
Landau’s book provides a counter-balance to Frankl, urging us not to be
perfectionists when finding meaning in our lives. We don’t need to be
curing cancer or solving world poverty to be able to call our lives meaningful. Instead,
we should be grateful for the meaning already in our lives found through our relationships
and whatever impact, no matter how small, our work may have.</p>

<h2 id="2-the-person-and-the-situation-lee-ross-and-richard-nisbett">2. <em>The Person and the Situation</em>, Lee Ross and Richard Nisbett</h2>

<p>I read this book because one of my favourite authors, Malcolm Gladwell, names this
book as a major influence. It does an excellent job of giving an overview
accessible to the general public of the research in social psychology while also
providing sufficient detail and references to dig more deeply on your own. You
can feel the excitement through the page as the authors outline future work to be done in the field
and all the areas in which their research could help improve society.</p>

<h2 id="3-the-power-broker-robert-caro">3. <em>The Power Broker</em>, Robert Caro</h2>

<p>Robert Caro is incredibly talented at making bureaucracy exciting to read about.
I find it satisfying to read all the details Caro has painstakingly dug up through his research.
It’s fascinating to read about the ambition and work ethic Bob Moses had and incredibly
depressing to realize the catastrophic effect it’s had on New York City’s transportation system today.
I’m currently reading the fourth book in <em>The Years of Lyndon Johnson</em> series and have been enjoying
that series even more.</p>

<h2 id="4-the-emperor-of-all-maladies-siddhartha-mukherjee">4. <em>The Emperor of All Maladies</em>, Siddhartha Mukherjee</h2>

<p>I enjoyed Mukherjee’s literary style and his method of revealing the history of cancer
through narrative. I also liked his commentary on the War on Cancer and the implications
and reasons behind it’s failure to produce results akin to other major government projects such as Apollo.</p>

<h2 id="5-models-of-my-life-herbert-simon">5. <em>Models of My Life</em>, Herbert Simon</h2>

<p>I liked reading about his days in undergrad at the University of Chicago;
I’m currently reading <em>The Second Mountain</em> by David Brooks and he speaks similarly
about the special environment at that institution. I was surprised at just how
adventurous Herbert Simon was and enjoyed reading about his travels. He writes
candidly about his relationship with his parents, his kids, and his wife, which
I found illuminating.</p>

<h2 id="6-i-will-teach-you-to-be-rich-ramit-sethi">6. <em>I Will Teach You to be Rich</em>, Ramit Sethi</h2>

<p>The title of this book is off-putting, which is unfortunate since this book
is well done. It has a bit of a self-help flavour, but is self-aware and tuned
to its target audience. I knew nothing about personal finance going in and
found this book immensely helpful in getting me excited about money while also
making me more aware of my own faults and emotions about it.</p>

<h2 id="7-understanding-power-noam-chomsky">7. <em>Understanding Power</em>, Noam Chomsky</h2>

<p>This book is known as a gentle introduction to Chomsky’s work as it’s in the format of
transcripts of talks/discussions he’s given instead of essays.
I personally would have preferred something more detailed to convince me of his theories;
I don’t like the conspiratorial (yes, I looked that word up) tone of the book. I
liked the personal comments included in the book; here’s one which has particularly stuck with me:</p>

<blockquote>
  <p>One choice is to assume the worst, and then you can be guaranteed that it’ll happen. The other is to assume that there’s some hope for change, in which case it’s possible that you can help to effect change. So you’ve got two choices, one guarantees the worst will happen, the other leaves open the possibility that things might get better. Given those choices, a decent person doesn’t hesitate.</p>
</blockquote>

<h2 id="8-personal-history-katharine-graham">8. <em>Personal History</em>, Katharine Graham</h2>

<p>I found it fascinating to read about the character and accomplishments of Graham’s parents,
and the corresponding values and expectations pressed upon her during her upbringing.
As it says in the title, this memoir is quite personal. I found it heart-wrenching to
read about her husband’s mental illness and the impact it had on her and her children.
It’s incredibly inspiring how she pulled herself out of that to wield
immense power and become a trailblazer for women’s rights in the workplace.</p>

<hr />

<p>I’ve managed to maintain my reading habit this semester for the first time
in my undergrad, so hopefully I can keep it up and read some more interesting books
this upcoming year.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2019/12/31/books/</guid>
                <description>
                    
                    The books which influenced me the most this year.
                    
                </description>
                <pubDate>Tue, 31 Dec 2019 00:00:00 -0800</pubDate>
                <author>Ziyue Yang</author>
            </item>
        
    
        
            <item>
                <title>Learning to Sort with a Neural Network</title>
                <link>http://localhost:4000/2018/06/20/sorting-numbers-with-a-neural-network/</link>
                <content:encoded>
                    <![CDATA[
                    <p>I recently came across an interesting challenge: sorting numbers with a neural
network<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. In contrast to other machine learning problems, this one is already
solved by classical methods (e.g. quicksort). And of course, the data we could
generate to feed to a machine learning model to learn how to sort is endless, so
what makes this problem interesting is how <em>little</em> data a machine learning
model would need to learn how to sort.</p>

<h2 id="generalization-to-longer-sequences">Generalization to Longer Sequences</h2>
<p>One of the most useful properties we could have would be for the model to learn
an algorithm which generalizes to longer input sequences. In particular, the
actual model architecture would readily need to have the capacity to generalize
to longer sequences, and the learning algorithm which trains it would have to
regularize appropriately so that the model does not overfit to the shorter
sequences it sees during training.</p>

<h2 id="a-naive-approach">A Naive Approach</h2>
<p>This problem obviously requires us to learn a function which maps sequences to
sequences, and an obvious way to perform this will be using an encoder-decoder
RNN architecture, as used for translation problems. One can do better by using
a common variant of this architecture: augmentation via an attention mechanism.
But can we do even better?</p>

<h1 id="an-improvement">An Improvement</h1>

<h2 id="outputs-correspond-directly-to-inputs">Outputs correspond directly to inputs</h2>
<p>In contrast to many sequence to sequence problems, the output of the model
will in fact be elements of the input. Many sequence to sequence problems are
some sort of translation between domains, and hence, obviously, outputs will
correspond to inputs <em>roughly</em> in a one-to-one manner, depending on the domain.
But in sorting, outputs correspond to inputs <em>precisely</em> in a one-to-one manner,
i.e. the output is a permutation of the input.</p>

<h2 id="pointer-networks">Pointer Networks</h2>
<p>The issue with returning a prediction corresponding to one of the input elements
is that the there are a <em>variable number</em> of them depending on the specific
example. However, it turns out we can use a simple trick to modify our attention
mechanism slightly so that we can get a prediction over the input elements. This
architecture, called a Pointer Network, is described in much more detail in the
<a href="https://arxiv.org/pdf/1506.03134.pdf">original paper</a>.</p>

<h1 id="additional-observations">Additional Observations</h1>
<p>There are in fact a lot of other interesting ideas one might apply to this
problem. Here are some speculations and ideas that I had that I haven’t yet
gotten the chance to try out.</p>

<h2 id="invariance-to-permutation">Invariance to Permutation</h2>
<p>One crucial invariance in the sorting problem, in contrast to most other
sequence to sequence problems (e.g. translation), is
that the order of the inputs does not affect the outcome. Or, as
<a href="http://fastml.com/introduction-to-pointer-networks/">FastML writes</a></p>

<blockquote>
  <p>In essence we’re dealing with sets, not sequences, as input. Sets don’t have
inherent order, so how elements are permuted ideally shouldn’t affect the
outcome.</p>
</blockquote>

<p>So, my comment earlier about us wanting to learn a function mapping from
sequence to sequence is actually not quite true. One approach to fixing this
problem might be including all possible permutations of a sequence, along with
the (same) output in the training set, essentially “emphasizing”<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> to the model
that the output should be invariant to permutation. However, this would be
extremely expensive (factorial growth with the length of the sequence) and might
not even work (i.e. “emphasize” to the model that order doesn’t matter).
Instead, it’d be much better if we could encode this invariance in the design of
the architecture itself. This is what the original authors of the Pointer
Network did, and you can read more about it
<a href="https://arxiv.org/pdf/1511.06391.pdf">here</a>.</p>

<h2 id="elastic-weight-consolidation">Elastic Weight Consolidation</h2>
<p>I recently read the paper describing
<a href="https://arxiv.org/pdf/1612.00796.pdf">this method</a> to do incremental
learning<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>, and I thought I might use it to try to help the network generalize
to longer sequences. In particular, one could use a form of curriculum learning:
incrementally teach the model to sort longer and longer sequences, (using EWC to
“overcome catastrophic forgetting”) and this might lead the network to better
generalization (unfortunately I haven’t been able to do any experiments yet).</p>

<h2 id="amount-of-computation-at-each-step">Amount of computation at each step</h2>
<p>A sequence to sequence model will, at each step, perform the same number of
operations. Even simple sorting algorithms like insertion sort perform different
amounts of computation at each step. It seems unreasonable for the decoder in
our architecture to perform the same amount of steps whether it’s supposed to
spit out 100 more numbers or 5. Once again, it turns out someone has already
thought of this: it’s called
<a href="https://arxiv.org/pdf/1603.08983.pdf">Adaptive Computation Time</a>
and you can read more about it on
<a href="https://distill.pub/2016/augmented-rnns/#adaptive-computation-time">distill</a> as
well.</p>

<h1 id="conclusion">Conclusion</h1>

<p>There is still plenty of potential future work to be done. More experiments
should be performed looking at generalization to longer sequences, the
performance of the models against the permutation metric and the nondecreasing
metric<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>, and coming up with other metrics and visualizations to better
understand how to further improve these models.</p>

<p>Even a simple toy problem like sorting turns out to spark a lot of interesting
intuitions, and led me to discover a lot of cool new architectures and
optimization schemes.</p>

<hr />
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>In particular sorting a finite set of integers. (Or, if one wanted to be really pedantic, an arbitrary totally ordered finite set) <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>It turns out there’s actually a name for this intuition of hand crafting the examples one feeds to a model: curriculum learning. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Continually learning new tasks without forgetting old ones. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>i.e. permutation metric being whether or not the network’s output is permutation of the input, and nondecreasing metric being whether or not the network’s output is nondecreasing sequence (i.e. “sorted”) <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2018/06/20/sorting-numbers-with-a-neural-network/</guid>
                <description>
                    
                    Exploring how one might teach a neural net to sort numbers.
                    
                </description>
                <pubDate>Wed, 20 Jun 2018 00:00:00 -0700</pubDate>
                <author>Ziyue Yang</author>
            </item>
        
    
  </channel>
</rss>
